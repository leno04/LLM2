{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "filenames = ['AR22.pdf', 'AR21.pdf', 'AR20.pdf','AR19.pdf', 'AR18-19.pdf', 'AR17-18.pdf','AR16-17.pdf','AR15-16.pdf','AR14-15.pdf','AR13-14.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def batch_translate_hindi_to_english(hindi_segments):\n",
    "    \"\"\"\n",
    "    Translate a batch of Hindi segments to English.\n",
    "    \"\"\"\n",
    "    translator = GoogleTranslator(source='hi', target='en')\n",
    "    translations = translator.translate_batch(hindi_segments)\n",
    "    return translations\n",
    "\n",
    "def translate_hindi_to_english(text):\n",
    "    hindi_pattern = re.compile(r'[\\u0900-\\u097F]+(?:[\\u0900-\\u097F\\s,;]*)')  \n",
    "    # Matches Hindi text and adjacent punctuation/spaces\n",
    "    hindi_segments = []\n",
    "    last_end = 0\n",
    "\n",
    "    # Collect Hindi segments\n",
    "    for match in hindi_pattern.finditer(text):\n",
    "        hindi_segments.append(match.group())\n",
    "\n",
    "    # Batch translate Hindi segments\n",
    "    if hindi_segments:\n",
    "        translated_segments = batch_translate_hindi_to_english(hindi_segments)\n",
    "    else:\n",
    "        translated_segments = []\n",
    "\n",
    "    # Reconstruct the text with translations\n",
    "    translated_text = []\n",
    "    hindi_index = 0\n",
    "    \n",
    "    for match in hindi_pattern.finditer(text):\n",
    "        # Append the text before the Hindi segment\n",
    "        translated_text.append(text[last_end:match.start()])\n",
    "        \n",
    "        # Append the translated segment\n",
    "        translated_text.append(translated_segments[hindi_index])\n",
    "        hindi_index += 1\n",
    "        \n",
    "        last_end = match.end()\n",
    "    \n",
    "    # Append any remaining text after the last Hindi segment\n",
    "    translated_text.append(text[last_end:])\n",
    "    \n",
    "    return ''.join(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path):\n",
    "    tables = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            extracted_tables = page.extract_tables()\n",
    "            for table in extracted_tables:\n",
    "                tables.append(table)\n",
    "    \n",
    "    return tables\n",
    "\n",
    "def preprocess_tables(tables):\n",
    "    # remove icar table and [''] tables\n",
    "    tables = tables[9:]\n",
    "    for table in tables:\n",
    "        if 'ICAR-CRIDA Annual Re\\n' in table[0][0] or table == [[''],['']]:\n",
    "            tables.remove(table)\n",
    "    return tables\n",
    "\n",
    "def table_to_text(df):\n",
    "    text_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        row_text = \" | \".join([f\"{col}: {row[col]}\" for col in df.columns])\n",
    "        text_data.append(row_text)\n",
    "    return text_data\n",
    "\n",
    "table_chunks=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def chunk_text_with_sentences(preprocessed_text, max_tokens=105):\n",
    "    tokens = word_tokenize(preprocessed_text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for token in tokens:\n",
    "        current_chunk.append(token)\n",
    "        if len(current_chunk) >= max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "\n",
    "    if current_chunk:  # If there are remaining tokens\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def extract_and_preprocess_from_pdf(pdf_path):\n",
    "    try:\n",
    "        # Open the PDF file\n",
    "        doc = fitz.open(pdf_path)\n",
    "        # Extract text from each page\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        text = translate_hindi_to_english(text)\n",
    "        \n",
    "        # Remove unwanted characters\n",
    "        text = re.sub(r'[^\\w\\s\\d]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "            print(f\"Failed to extract text from {pdf_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "def embed(text_chunks, nums, model_name='gpt2', device='cpu'):\n",
    "    # Load pre-trained GPT-2 tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2Model.from_pretrained(model_name)\n",
    "    # Move model to specified device\n",
    "    model.to(device)\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and encode text chunks\n",
    "    encoded_chunks = []\n",
    "    for chunk in text_chunks:\n",
    "        # Tokenize the chunk\n",
    "        tokens = tokenizer.encode(chunk)\n",
    "        nums.append(len(tokens))\n",
    "        # Truncate tokens if exceeds max length\n",
    "        if len(tokens) > tokenizer.model_max_length:\n",
    "            tokens = tokens[:tokenizer.model_max_length]\n",
    "        # Convert tokens to PyTorch tensor and add to encoded_chunks list\n",
    "        encoded_chunks.append(torch.tensor(tokens).unsqueeze(0))\n",
    "\n",
    "    # Generate embeddings\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for chunk in encoded_chunks:\n",
    "            chunk = chunk.to(device)\n",
    "            outputs = model(chunk)\n",
    "            # Extract hidden states (last hidden state)\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            # Take mean of hidden states across tokens to get chunk embedding\n",
    "            chunk_embedding = torch.mean(hidden_states, dim=1)\n",
    "            embeddings.append(chunk_embedding)\n",
    "\n",
    "    # Concatenate embeddings along the first dimension to get a single tensor\n",
    "    embeddings_tensor = torch.cat(embeddings, dim=0)\n",
    "    \n",
    "    return embeddings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\LLM1\\\\webs\\\\data\"\n",
    "text_chunks = []\n",
    "nums = []\n",
    "allembeddings = []\n",
    "for i in filenames:\n",
    "    text = extract_and_preprocess_from_pdf(file_path+\"\\\\\"+i)\n",
    "    chunks = chunk_text_with_sentences(text)\n",
    "    text_chunks.extend(chunks)\n",
    "    \n",
    "    tables = extract_tables_from_pdf(file_path+\"\\\\\"+i)\n",
    "    tables = preprocess_tables(tables)\n",
    "    for t in range(len(tables)):\n",
    "        table = tables[t]\n",
    "        data = {}\n",
    "        keys = table[0]\n",
    "        table.pop(0)\n",
    "        table = np.array(table)\n",
    "        for j,k in enumerate(keys):\n",
    "            data[k] = table[:,j]\n",
    "        df = pd.DataFrame(data)\n",
    "        text_chunks.extend(table_to_text(df))\n",
    "    break\n",
    "nums = []\n",
    "allembeddings.extend(embed(text_chunks,nums))\n",
    "print(len(text_chunks))\n",
    "print(len(allembeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282\n",
      "1239\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "print(max(nums))\n",
    "print(len(allembeddings))\n",
    "print(len(allembeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage\n",
    "\n",
    "import pickle\n",
    "# import fitz\n",
    "\n",
    "# Saving allembeddings and token_chunks\n",
    "with open('allembeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(allembeddings, f)\n",
    "\n",
    "with open('text_chunks.pkl', 'wb') as f:\n",
    "    pickle.dump(text_chunks, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allembeddings= 1239\n",
      "loaded_textchunks= 1239\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('allembeddings.pkl','rb') as f:\n",
    "    loaded_allembeddings = pickle.load(f)\n",
    "with open('text_chunks.pkl', 'rb') as f:\n",
    "    loaded_textchunks = pickle.load(f)\n",
    "print(\"allembeddings=\",len(loaded_allembeddings))\n",
    "print(\"loaded_textchunks=\",len(loaded_textchunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 972, 979, 47, 960]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_nearest_embeddings(query_embedding, model_name='gpt2', device='cpu', top_k=5):\n",
    "    \n",
    "    with open('allembeddings.pkl','rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "\n",
    "    # Convert lists of tensors to a single tensor\n",
    "    # query_embedding = torch.stack(query_embedding).to(device)\n",
    "    embeddings = torch.stack(embeddings).to(device)\n",
    "\n",
    "    # Move tensors to the specified device\n",
    "    query_embedding = query_embedding.to(device)\n",
    "    embeddings = embeddings.to(device)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    similarities = cosine_similarity(query_embedding.cpu().numpy(), embeddings.cpu().numpy())[0]\n",
    "\n",
    "    # Find the top_k nearest embeddings\n",
    "    top_k_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    # top_k_similarities = similarities[top_k_indices]\n",
    "\n",
    "    return top_k_indices.tolist()\n",
    "\n",
    "query_nums=[]\n",
    "query = \"Who is the current director of ICAR-CRIDA?\"\n",
    "# print(embed([query],query_nums)[0])\n",
    "nearest_ids = find_nearest_embeddings(embed([query],query_nums), model_name='gpt2', device='cpu', top_k=5)\n",
    "print(nearest_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  Who is the current director of ICAR-CRIDA?\n",
      "0 and recognition from national academies professional societies and other institutions97 Research ArticleInternationalNational peerreviewed journals Other Publications Books Reports Bulletins Training Manuals Brochures40 Book Chapters published ICARKREEDA is working towards providing technological solutions for rainfed agriculture in the country through collaboration of Ministries and Departments of Government of India State Governments State Agricultural Universities NonGovernment Agricultural Research Institutes etcThe Government continues to work closely with organisations and private industry I express my gratitude to the Indian Council of Agricultural Research for its continued support and guidance I commend the work done by the Annual Report Committee in preparing this report HyderabadHyderabad Vinod Kumar Singh May2023 ICARCRIDA\n",
      "1 Scientist: None | Details of program: Re-interaction Meet on Energy – Water – Food Nexus. Challenges\n",
      "Ahead – Climate change, IPCC Report at VIP circuit House Annexe,\n",
      "Sahibagh, as speaker on \"Energy – Water – Food Nexus Challenges\n",
      "Ahead | Duration: October 7,\n",
      "2022 | Venue: Ahmadabad,\n",
      "Gujarat\n",
      "2 Scientist: None | Details of program: Visited AICRPDA centre, Bengaluru to review on-station and on-farm\n",
      "activities under AICRPDA, NICRA, OFR and SCSP. | Duration: November 27-\n",
      "28, 2022 | Venue: Bengaluru,\n",
      "Karnataka\n",
      "3 various livelihood activities for women in agriculture Human Resource Development During the year as a part of human resource development15 Scientists5 Technical and2 Finance employees participated in various training programmes Participation of employees in conferences meetings workshops seminars and symposia During the year the scientists of the Institute100 Attended various conferences meetings workshops seminars and symposia Awards and recognitions The Institute received three awards and sixteen scientists received awards fellowships and recognitions from national academies professional societies and other institutions PublicationsSoftwareWebsiteDatabase Ninetyseven research articlesInternationalNational Peer Reviewed Journals Other Publications Books Reports Bulletins Training Manuals Brochures40 Book chapters were published and mobile apps were also developed\n",
      "4 Scientist: None | Details of program: Consultation meeting on Glue Grant by the Secy, Min of Education,\n",
      "Govt. of India | Duration: May 2, 2022 | Venue: Virtual\n",
      "context:  and recognition from national academies professional societies and other institutions97 Research ArticleInternationalNational peerreviewed journals Other Publications Books Reports Bulletins Training Manuals Brochures40 Book Chapters published ICARKREEDA is working towards providing technological solutions for rainfed agriculture in the country through collaboration of Ministries and Departments of Government of India State Governments State Agricultural Universities NonGovernment Agricultural Research Institutes etcThe Government continues to work closely with organisations and private industry I express my gratitude to the Indian Council of Agricultural Research for its continued support and guidance I commend the work done by the Annual Report Committee in preparing this report HyderabadHyderabad Vinod Kumar Singh May2023 ICARCRIDA Scientist: None | Details of program: Re-interaction Meet on Energy – Water – Food Nexus. Challenges\n",
      "Ahead – Climate change, IPCC Report at VIP circuit House Annexe,\n",
      "Sahibagh, as speaker on \"Energy – Water – Food Nexus Challenges\n",
      "Ahead | Duration: October 7,\n",
      "2022 | Venue: Ahmadabad,\n",
      "Gujarat Scientist: None | Details of program: Visited AICRPDA centre, Bengaluru to review on-station and on-farm\n",
      "activities under AICRPDA, NICRA, OFR and SCSP. | Duration: November 27-\n",
      "28, 2022 | Venue: Bengaluru,\n",
      "Karnataka various livelihood activities for women in agriculture Human Resource Development During the year as a part of human resource development15 Scientists5 Technical and2 Finance employees participated in various training programmes Participation of employees in conferences meetings workshops seminars and symposia During the year the scientists of the Institute100 Attended various conferences meetings workshops seminars and symposia Awards and recognitions The Institute received three awards and sixteen scientists received awards fellowships and recognitions from national academies professional societies and other institutions PublicationsSoftwareWebsiteDatabase Ninetyseven research articlesInternationalNational Peer Reviewed Journals Other Publications Books Reports Bulletins Training Manuals Brochures40 Book chapters were published and mobile apps were also developed Scientist: None | Details of program: Consultation meeting on Glue Grant by the Secy, Min of Education,\n",
      "Govt. of India | Duration: May 2, 2022 | Venue: Virtual\n",
      "answer:  The current Director of International Research in India is Dr. Raman Singh.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pickle\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def generate_answer(query: str, context: str, model_name: str = 'gpt2', max_length: int = 1024) -> str:\n",
    "\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    # Prepare the input text\n",
    "    input_text = f\"Context: {context}\\n\\nQuery: {query}\\n\\nAnswer:\"\n",
    "    \n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Generate the output\n",
    "    output_ids = model.generate(input_ids, \n",
    "                                max_length=max_length, \n",
    "                                num_return_sequences=1,\n",
    "                                no_repeat_ngram_size=2, \n",
    "                                temperature=0.3,\n",
    "                                top_p=0.9,\n",
    "                                do_sample=True,\n",
    "                                pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Decode the output\n",
    "    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Post-process the output to extract the answer part\n",
    "    # answer = answer.split(\"Answer:\")[1].strip()\n",
    "    answer = answer.split(\"Answer:\")[1].strip().split(\"\\n\")[0]\n",
    "\n",
    "    # Tokenize the answer into sentences\n",
    "    sentences = sent_tokenize(answer)\n",
    "\n",
    "    # Reconstruct the answer without the last incomplete sentence\n",
    "    complete_answer = ' '.join(sentences[:-1]) if not answer.endswith('.') else answer\n",
    "    return complete_answer\n",
    "\n",
    "\n",
    "def get_context(nearest_ids):\n",
    "    with open('text_chunks.pkl', 'rb') as f:\n",
    "        loaded_chunks = pickle.load(f)\n",
    "\n",
    "    # nearest_ids.sort()\n",
    "    chunks = [loaded_chunks[i] for i in nearest_ids]\n",
    "    for i,j in enumerate(chunks):\n",
    "        print(i,j)\n",
    "    context = \" \".join([i for i in chunks])\n",
    "    print(\"context: \", context)\n",
    "    return context\n",
    "\n",
    "print(\"Query: \",query)\n",
    "context = get_context(nearest_ids)\n",
    "answer = generate_answer(query, context)\n",
    "print(\"answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
